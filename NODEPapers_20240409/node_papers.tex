\documentclass[usenames,dvipsnames]{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
% \usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
% \usetheme{Berkeley}
% \usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
% \usetheme{Copenhagen}
\usetheme{Darmstadt}
% \usetheme{Dresden}
% \usetheme{Frankfurt}
% \usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
% \usetheme{Madrid}
% \usetheme{Malmoe}
%\usetheme{Marburg}
% \usetheme{Montpellier}
% \usetheme{PaloAlto}
% \usetheme{Pittsburgh}
%\usetheme{Rochester}
% \usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

% \usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

% % -------------------------------------------
% % Songkai added, feel free to delete --------
% \AtBeginSection[]{
%   \begin{frame}
%   \vfill
%   \centering
%   \begin{beamercolorbox}[sep=8pt,center,shadow=false,rounded=true]{title}
%     \usebeamerfont{title}\insertsectionhead\par%
%   \end{beamercolorbox}
%   \vfill
%   \end{frame}
% }
% % Songkai added, feel free to delete --------
% % -------------------------------------------




\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{natbib}
\usepackage{amsmath, amssymb, graphicx, url}
\usepackage[ruled]{algorithm2e}
\usepackage{commath}
\usefonttheme[onlymath]{serif}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{centernot}
\usepackage{comment}
%\usepackage[a4paper, margin=0.8in]{geometry}
\usepackage{parskip}
\usepackage{graphicx}

\usepackage{natbib}

\usepackage{tikz}
\usepackage{tikzlings}

\usepackage{tabularx}
\usepackage{array}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{mathtools}
\usepackage{bm,upgreek}
\usepackage{subcaption}
\usepackage{textpos}
% \usepackage{eso-pic}

% \usepackage{multimedia}
\usepackage{media9}

\def\E{\mathbf{E}}
\def\PP{\mathbf{P}}
\def\Reals{\mathbb{R}}
\def\Naturals{\mathbb{N}}
\def\argmin{\operatornamewithlimits{arg\,min}}
\def\deq{:=}
\def\wh#1{\widehat{#1}}
\def\bd#1{\mathbf{#1}}
\def\bx{\bd{x}}
\def\by{\bd{y}}
\def\bZ{\bd{Z}}
\def\bB{\bd{B}}
\def\bV{\bd{V}}
\def\tO{{\tilde{\cO}}}
\def\tOm{\tilde{\Omega}}
\def\barw{\overline{w}}
\def\d{{\mathrm d}}
\def\ave#1{\langle #1 \rangle}
\def\Ave#1{\left\langle #1 \right\rangle}
\def\eps{\varepsilon}
\def\tr{\mathrm{Tr}}


\def\HS{\mathbb{H}}
\def\reals{\mathbb{R}}
\def\ths{\theta^*}
\def\thh{\hat{\theta}}
\def\lbr{\left[}
\def\rbr{\right]}
\def\lc{\left(}
\def\rc{\right)}


    \def\ddefloop#1{\ifx\ddefloop#1\else\ddef{#1}\expandafter\ddefloop\fi}
    % \cA, \cB, ...
    \def\ddef#1{\expandafter\def\csname c#1\endcsname{\ensuremath{\mathcal{#1}}}}
    \ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop
    \def\argmin{\operatornamewithlimits{arg\,min}}
    \def\E{\mathbf{E}}
    \def\bx{\bd{x}}
	\def\by{\bd{y}}
    \def\bZ{\bd{Z}}

\newcommand{\propnumber}{} % initialize
\newtheorem*{prop}{Proposition \propnumber}
\newenvironment{propc}[1]
  {\renewcommand{\propnumber}{#1}%
   \begin{shaded}\begin{prop}}
  {\end{prop}\end{shaded}}

\newcommand{\crlrnumber}{} % initialize
%\newtheorem*{corollary}{Corollary \crlrnumber}
\newenvironment{corollaryc}[1]
  {\renewcommand{\crlrnumber}{#1}%
   \begin{shaded}\begin{corollary}}
  {\end{corollary}\end{shaded}}

\theoremstyle{definition}
% \newtheorem{definition}




% \setbeamertemplate{headline}{% 
%     \leavevmode%
%     \hbox{%
%         \begin{beamercolorbox}[wd=.4\paperwidth,ht=2.25ex,dp=1ex,right]{section in head/foot}%
%             \usebeamerfont{section in head/foot}\insertshorttitle\hspace*{2ex}
%         \end{beamercolorbox}%
%         \begin{beamercolorbox}[wd=.6\paperwidth,ht=2.25ex,dp=1ex,left]{subsection in head/foot}%
%             \usebeamerfont{section in head/foot}\includegraphics[height=2ex,keepaspectratio]{Slides/Block_M-Hex.png}\hspace*{2ex}\insertsectionhead
%         \end{beamercolorbox}%
%     }
% }

% \addtobeamertemplate{headline}{}{%
% \begin{textblock*}{100mm}(.85\textwidth,-1cm)
% \Huge\textcolor{white}{\textbf{\TeX}}
% \end{textblock*}}



%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

%%%% TRY WITH TEXTPOS
% \newcommand{\imgblock}{\begin{textblock*}{5cm}(10.5cm,-1.2cm) % {block width} (coords)
%         \includegraphics[width=1cm]{Slides/Block_M-Hex.png} % loading the image
%     \end{textblock*}
%     }

% \addtobeamertemplate{background}{\imgblock}{}



% \setbeamertemplate{headline}{\hfill\includegraphics[width=1.5cm]{Slides/Block_M-Hex.png}\hspace{0.2cm}\vspace{-1cm}}

% \logo{\includegraphics[height=1cm]{Slides/Block_M-Hex.png}}

\addtobeamertemplate{frametitle}{}{%
    \begin{textblock*}{5cm}(10.5cm, -0.8cm)
        \includegraphics[width=0.9cm]{Block_M-Hex.png} % your logo file here
    \end{textblock*}
}

\definecolor{mycolor}{cmyk}{100, 60, 0, 60}
\definecolor{my_maize}{rgb}{0.9608,0.7137,0.2588}
\definecolor{my_yellow}{rgb}{0.9294,0.8196,0.2706}

% \setbeamercolor{section in head/foot}{fg=cyan}
\setbeamercolor{section in head/foot}{fg=my_maize}

\setbeamercolor{frametitle}{bg=mycolor}
\setbeamercolor{titlelike}{fg=black, bg=yellow}

\usepackage{url}
\usepackage{hyperref}

\usepackage{xcolor}

\hypersetup{pdfauthor={Name},
            colorlinks=true,
            linkcolor={my_yellow},
            % citecolor={blue},
            % linkcolor=[RGB]{0.949, 0.784, 0.035}
            }

% fix inconsistent colors in cite parenthesis (where the closing parenthesis were black instead of the rest of the citecolor!)
% https://github.com/josephwright/beamer/issues/671
% \let\oldcite=\cite
% \let\oldcitet=\citet
% \let\oldcitep=\citep 
% \renewcommand{\citet}[2][]{\textcolor{green}{\oldcitet[#1]{#2}}}
% \renewcommand{\citep}[2][]{\textcolor{green}{\oldcitep[#1]{#2}}}
% \renewcommand{\cite}[2][]{\textcolor{green}{\oldcite[#1]{#2}}}
            

\title[Seminar]{Papers: ResNets, CNNs, and NODEs}
% \title[Seminar]{An Adaptive Bayesian Method for Covariance Estimation in Multifidelity Estimators}

% and Estimation of Predictive Uncertainties
% The short title appears at the bottom of every slide, the full title is only on the title page

\author[AJ]{Aniket Jivani}
% \institute[U-M]{Department of Mechanical Engineering, \\ University of Michigan}

\date{\today}

% \AtBeginSection[]
% {
%  \begin{frame}<beamer>
%  \frametitle{Plan}
%  \tableofcontents[currentsection]
%  \end{frame}
% }


\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}


% \begin{frame}
%     \frametitle{Overview} % Table of contents slide, comment this block out to remove it
%     \tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
% \end{frame}

\begin{frame}{Overview}

% Sander et al. \url{https://openreview.net/pdf?id=1tCuRbPts3J}

% Poli et al. \url{https://arxiv.org/pdf/1911.07532.pdf}
\textbf{Motivation for self:} Think a little more about selecting suitable neural network architectures for scientific data in the small data setting.

Brief descriptions - 

\begin{enumerate}
    \item Initial connection between Deep Neural Networks and different families of PDEs \href{https://arxiv.org/abs/1804.04272}{Ruthotto and Haber 2018}
    
    \item Reframing neural operators as implicit layers \href{https://arxiv.org/abs/2203.08205}{You et al., 2022}
    
    \item Neural ODEs as the true deep limit of ResNets \href{https://arxiv.org/abs/2002.08071}{Massaroli et al., 2021}
\end{enumerate}

\end{frame}

\section{Part I}
\begin{frame}{PDEs meet Image Processing}
    
    (For many decades, in different applications)

    % \pause

    \emph{Optical Flows...}(estimate motion field from time-varying intensities)
        $$\frac{\partial I}{\partial x}V_x + \frac{\partial I}{\partial y}{V_y} + \frac{\partial{I}}{\partial{t}} = 0$$
    % \pause

    \emph{Anisotropic diffusion...}(\cite{perona_scale-space_1990}) - \texttt{imdiffusefilt} in MATLAB
    $$\frac{\partial I}{\partial t} = \operatorname{div}(c(x, y, t) \nabla (I))$$

    Gradually moved on to \textcolor{red}{`less inductive bias, more data'} approach for all kinds of problems!
\end{frame}



\begin{frame}{Parametrizing Convolutional Operators}
    Write ResNet as a forward Euler discretization with a fixed step size:

    $$Y_{j+1} = Y_j + F(\theta^j, Y_j) \implies \partial_t\mathbf{Y}(\boldsymbol{\theta},t)&=\mathbf{F}(\boldsymbol{\theta}(t),\mathbf{Y}(t)),\mathrm{~for~}t\in(0,T]$$

    $$\mathbf{Y}(\boldsymbol{\theta},0)&=\mathbf{Y}_0$$


    $F$ is typically composed of: $\mathbf{K}_2(\theta^{(3)})\sigma\left(\mathcal{N}(\mathbf{K}_1(\boldsymbol{\theta}^{(1)})\mathbf{Y},\boldsymbol{\theta}^{(2)})\right)$ where $\mathcal{N}$ is normalization layer and $\mathbf{K}$ are parametrized by stencils $\theta$.
\end{frame}

\begin{frame}{Examples of stencils}

A familiar one is for the Laplacian in the heat equation: $\theta = [1, -2, 1]/h^2$. For first order centered differences, use: $[-1, 0, 1]/2h$ and so on.

Weighted $\theta$ lead us to:

$$\mathbf{K}_1(\theta)=\beta_1(\theta)+\beta_2(\theta)\partial_x+\beta_3(\theta)\partial_x^2$$

Multiply by different sizes and values of stencils to generate other differential operators.

The above implies learned weights depend strongly on  image resolution! i.e. $h$.

\textcolor{red}{Exploit:} Multiscale training strategies! \cite{haber_learning_2017}

start from coarse mesh operator $K_H$, train network and solve for fine operator $K_h$ from $K_H = R K_h P$ - leads to training efficiencies.
    
\end{frame}


\begin{frame}{Similar Ideas in Neural Operators}
    Other works, too, are converging on these ideas... e.g. \cite{liu-schiaffini_neural_2024} \url{https://arxiv.org/pdf/2402.16845.pdf}

    This proposes a discretization invariant version of the differential layer.

\end{frame}


\begin{frame}{Hyperbolic systems}
    (show parabolic CNNs from paper)

    e.g. wave equation, Burgers equation,
    generalized Burgers:

    $$\frac{\partial u}{\partial t} + c(u) \frac{\partial{u}}{\partial{x}} = \nu \frac{\partial ^2 u}{\partial x^2}$$

    (typically successful regularizations involve adding some amount of dissipation on the RHS for physically acceptable solutions)

    Parabolic vs hyperbolic nature dictated by strengths of respective terms. For linear transport ($\partial_t c + a \partial_x c = 0$ - the points away from the discontinuity are unchanged till the wave reaches that part of the domain.)

    \textcolor{red}{Finite vs infinite speed of propagation of information}
    
\end{frame}

\begin{frame}{Hamiltonian mechanics POV}
    Total energy of system i.e. H(q(t), p(t)) is conserved. $q$, $p$ - position, momentum coordinates.

    Hamiltonian CNNs: In paper notation , introduce an auxiliary variable $Z$ (\textcolor{red}{partitioning channels of original features})

    $\partial_t\mathbf{Y}(t)=\mathbf{F}_\text{sym}(\boldsymbol{\theta}^{(1)}(t),\mathbf{Z}(t)),\quad\mathbf{Y}(0)=\mathbf{Y}_0$

    $\partial_t\mathbf{Z}(t)=-\mathbf{F}_\text{sym}(\boldsymbol{\theta}^{(2)}(t),\mathbf{Y}(t)),\quad\mathbf{Z}(0)=\mathbf{Z}_0.$

    and integrate:

    $\mathbf{Y}_{j+1}=\mathbf{Y}_j+\delta_t\mathbf{F}_{\mathrm{sym}}(\boldsymbol{\theta}^{(1)}(t_j),\mathbf{Z}_j) \,\mathbf{Z}_{j+1}=\mathbf{Z}_j-\delta_t\mathbf{F}_{\mathrm{sym}}(\boldsymbol{\theta}^{(2)}(t_j),\mathbf{Y}_{j+1})$


    Look for Hamiltonian $ +  X$ (ROM, NODE, etc.): \cite{greydanus_hamiltonian_2019,sharma_hamiltonian_2022,huh_time-reversal_2020}


    \textcolor{magenta}{Results:ResNets with 3x fewer parameters for comparable accuracy.}
\end{frame}


% \begin{frame}{What does this mean for ROMs?}

%     \item In the happy case that the underlying equations are known and can be simplified, we can rely on data-driven methods to find the speed and obtain a very simple reduced-order model / architecture.
    

%     for example, \cite{issan_predicting_2023}
        
%     $$\frac{\partial \tilde{u}}{\partial t} + \left(\tilde{u} + \frac{dc}{dt}\right) \frac{\partial \tilde{u}}{\partial \tilde{x}} = 0$$

%     Moving coordinates: $x \rightarrow x + c(t)$:

%     % $$\frac{\partial \tilde{u}}{\partial t} +  \frac{\partial}{\partial \tilde{x}}\left[g(\tilde{u})\right] = 0$$ where $g(\tilde{u}) = 1/2 (\tilde{u})^2  + (dc/dt) \tilde{u}$

%     \textcolor{red}{linear-quadratic ROM, find speed by cross-correlation}
% \end{frame}

% \begin{frame}{What about White Light Images?}
%     \begin{enumerate}
%         \item We can specify the intensity as a line-of-sight integral with very reasonable assumptions:
        

%     \end{enumerate}
    
% \end{frame}
% \begin{frame}{title}
    
% \end{frame}



\section{Part II}
\begin{frame}{Neural Operators TL;DR}



\end{frame}


\begin{frame}{Timestepping with Neural Operators}


\end{frame}

\begin{frame}{Implicit Layers}


\end{frame}


\begin{frame}{Fixed Point Iterations}



\end{frame}


% \begin{frame}{Paper 2 - Motivations}

% Original motivation: Can we leverage ideas from neural operators to perform \textcolor{red}{image-to-image mapping} in continuous time setting? (given lack of improvements in evolving latent-space dynamics)

% \textcolor{red}{\textbf{Tired:}} CNN (need for dense fully connected layers), CNN-encoder + NODE in latent space (terrible performance in reconstruction, lack of interpretability)

% \textcolor{violet}{\textbf{Wired:}} Neural Operators (convolutions, but fancier and actually work well for PDE-based problems)

% \pause

% While Neural operators have adopted the Fourier space as \emph{de facto standard}, their original motivation comes from GNNs, so this work is very relevant - Poli et al. \url{https://arxiv.org/pdf/1911.07532.pdf}

% % (the take of the Neural operator papers is - continuity is an artifical algorithmic depth parameter for NODE type approaches, while our method considers continuity of input and output data)

% \end{frame}

% \begin{frame}{Big takeway}
%     \textcolor{red}{Are Neural ODEs the true deep limit of ResNets? Not quite!}
%     Short explanation:
    
    
% \end{frame}

% \begin{frame}{Why does this matter?}
%     The reason is the need to separate the notion of \textcolor{blue}{the depth variable} in special cases of $f$ and the physical \textcolor{magenta}{time variable} where we are interested in the solution.

%     This leads to \emph{autoregressive GDE} type formulation:
% \end{frame}


% \begin{frame}{Autoregressive GDEs}

% \end{frame}

\section{Part III}
\begin{frame}{Depth Invariance}
    
\end{frame}


\begin{frame}{Connection to stacked ODEs}

\end{frame}

\begin{frame}
\textbf{Unclear:} When is it beneficial to pursue learning with continuous depth variables i.e. stacking of `ODESolve'? What would be the challenges in training this compared to discrete depth settings?


\textcolor{red}{How would this translate to making predictions over longer time horizons for different initial conditions? To test!}

\end{frame}

\begin{frame}{Key takeaways:}
    \begin{enumerate}
        \item There are interesting connections between PDE theory and treating networks as a continuous process over the depth parameter that can lead potentially to better architectures.
        
        \item Turbulence would have been a solved problem by now if only more people used convolutions ;) (and had many many GPUs)
        
        \item Plenty of work remaining in bridging the gap when there are no obvious ways to reduce or simplify models further, or multiple variables are coupled together.
        
        \item Reversibility! - \href{{https://math.stackexchange.com/questions/628720/why-heat-equation-is-not-time-reversible-time-arrow-in-mathematics}}{Heat Eqn vs Wave Eqn}, \href{https://doi.org/10.1016/S0167-2789(97)00199-1}{Lamb and Roberts} - arguments lead to TRS-NODEs \url{https://arxiv.org/abs/2007.11362}
    \end{enumerate}

\end{frame}
% \begin{frame}
%  \frametitle{Overview} % Table of contents slide, comment this block out to remove it
%  \tableofcontents % Throughout your presentation, if you choose to use \section{} and \subsection{} commands, these will automatically be printed on this slide as an overview of your presentation
% \end{frame}

\begin{frame}[allowframebreaks]
    \frametitle{References}
    \bibliographystyle{chicago}

    % \bibliographystyle{IEEEtran}
    \bibliography{myRef}
\end{frame}

% \section{Backup}
\begin{frame}{}
\begin{center}
    \Large{Thank you!}
\end{center}
        
\end{frame}

\end{document}
