@inproceedings{balandat2020botorch,
    title = {{BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization}},
    author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},
    booktitle = {Advances in Neural Information Processing Systems 33},
    year = 2020,
    url = {https://proceedings.neurips.cc/paper/2020/hash/f5b1b89d98b7286673128a5fb112cb9a-Abstract.html}
}


@phdthesis{frazier_knowledge-gradient_2009,
	address = {United States -- New Jersey},
	type = {Ph.{D}.},
	title = {Knowledge-gradient methods for statistical learning},
	copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
	url = {https://www.proquest.com/pqdtglobal/docview/304988451/abstract/7B4AD470733741DAPQ/1?sourcetype=Dissertations%20&%20Theses},
	abstract = {We consider the class of fully sequential Bayesian information collection problems, a class that includes ranking and selection problems, multi-armed bandit problems, and many others. Although optimal policies for such problems are generally known to exist and to satisfy Bellman's recursion, the curses of dimensionality prevent us from actually computing them except in a few very special cases. Motivated by this difficulty, we develop a general class of practical and theoretically well-founded information collection policies known as knowledge-gradient (KG) policies. KG policies have several attractive qualities: they are myopically optimal in general; they are asymptotically optimal in a broad class of problems; they are flexible and may be computed easily in a broad class of problems; and they perform well numerically in several well-studied ranking and selection problems compared with other state-of-the-art policies designed specifically for these problems.},
	language = {English},
	urldate = {2024-08-15},
	school = {Princeton University},
	author = {Frazier, Peter},
	year = {2009},
	keywords = {Statistics, Operations research, Applied sciences, Information collection, Knowledge gradients, Pure sciences, Statistical learning},
}

@inproceedings{astudillo_bayesian_2019,
	title = {Bayesian {Optimization} of {Composite} {Functions}},
	url = {https://proceedings.mlr.press/v97/astudillo19a.html},
	abstract = {We consider optimization of composite objective functions, i.e., of the form ùëì(ùë•)=ùëî(‚Ñé(ùë•))f(x)=g(h(x))f(x)=g(h(x)), where ‚Ñéhh is a black-box derivative-free expensive-to-evaluate function with vector-valued outputs, and ùëîgg is a cheap-to-evaluate real-valued function. While these problems can be solved with standard Bayesian optimization, we propose a novel approach that exploits the composite structure of the objective function to substantially improve sampling efficiency. Our approach models ‚Ñéhh using a multi-output Gaussian process and chooses where to sample using the expected improvement evaluated on the implied non-Gaussian posterior on ùëìff, which we call expected improvement for composite functions (EI-CF). Although EI-CF cannot be computed in closed form, we provide a novel stochastic gradient estimator that allows its efficient maximization. We also show that our approach is asymptotically consistent, i.e., that it recovers a globally optimal solution as sampling effort grows to infinity, generalizing previous convergence results for classical expected improvement. Numerical experiments show that our approach dramatically outperforms standard Bayesian optimization benchmarks, reducing simple regret by several orders of magnitude.},
	language = {en},
	urldate = {2024-08-15},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Astudillo, Raul and Frazier, Peter},
	month = may,
	year = {2019},
	pages = {354--363},
}

@misc{frazier_tutorial_2018,
	title = {A {Tutorial} on {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/1807.02811},
	doi = {10.48550/arXiv.1807.02811},
	abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Frazier, Peter I.},
	month = jul,
	year = {2018},
	note = {arXiv:1807.02811 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
}


@misc{wilson_maximizing_2018,
	title = {Maximizing acquisition functions for {Bayesian} optimization},
	url = {http://arxiv.org/abs/1805.10196},
	doi = {10.48550/arXiv.1805.10196},
	abstract = {Bayesian optimization is a sample-efficient approach to global optimization that relies on theoretically motivated value heuristics (acquisition functions) to guide its search process. Fully maximizing acquisition functions produces the Bayes' decision rule, but this ideal is difficult to achieve since these functions are frequently non-trivial to optimize. This statement is especially true when evaluating queries in parallel, where acquisition functions are routinely non-convex, high-dimensional, and intractable. We first show that acquisition functions estimated via Monte Carlo integration are consistently amenable to gradient-based optimization. Subsequently, we identify a common family of acquisition functions, including EI and UCB, whose properties not only facilitate but justify use of greedy approaches for their maximization.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Wilson, James T. and Hutter, Frank and Deisenroth, Marc Peter},
	month = dec,
	year = {2018},
	note = {arXiv:1805.10196 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Proceedings of the Thirty-second Conference on Neural Information Processing Systems, 2018},
}

@misc{miller_targeted_2024,
	title = {Targeted {Variance} {Reduction}: {Robust} {Bayesian} {Optimization} of {Black}-{Box} {Simulators} with {Noise} {Parameters}},
	shorttitle = {Targeted {Variance} {Reduction}},
	url = {http://arxiv.org/abs/2403.03816},
	doi = {10.48550/arXiv.2403.03816},
	abstract = {The optimization of a black-box simulator over control parameters \${\textbackslash}mathbf\{x\}\$ arises in a myriad of scientific applications. In such applications, the simulator often takes the form \$f({\textbackslash}mathbf\{x\},{\textbackslash}boldsymbol\{{\textbackslash}theta\})\$, where \${\textbackslash}boldsymbol\{{\textbackslash}theta\}\$ are parameters that are uncertain in practice. Robust optimization aims to optimize the objective \${\textbackslash}mathbb\{E\}[f({\textbackslash}mathbf\{x\},{\textbackslash}boldsymbol\{{\textbackslash}Theta\})]\$, where \${\textbackslash}boldsymbol\{{\textbackslash}Theta\} {\textbackslash}sim {\textbackslash}mathcal\{P\}\$ is a random variable that models uncertainty on \${\textbackslash}boldsymbol\{{\textbackslash}theta\}\$. For this, existing black-box methods typically employ a two-stage approach for selecting the next point \$({\textbackslash}mathbf\{x\},{\textbackslash}boldsymbol\{{\textbackslash}theta\})\$, where \${\textbackslash}mathbf\{x\}\$ and \${\textbackslash}boldsymbol\{{\textbackslash}theta\}\$ are optimized separately via different acquisition functions. As such, these approaches do not employ a joint acquisition over \$({\textbackslash}mathbf\{x\},{\textbackslash}boldsymbol\{{\textbackslash}theta\})\$, and thus may fail to fully exploit control-to-noise interactions for effective robust optimization. To address this, we propose a new Bayesian optimization method called Targeted Variance Reduction (TVR). The TVR leverages a novel joint acquisition function over \$({\textbackslash}mathbf\{x\},{\textbackslash}boldsymbol\{{\textbackslash}theta\})\$, which targets variance reduction on the objective within the desired region of improvement. Under a Gaussian process surrogate on \$f\$, the TVR acquisition can be evaluated in closed form, and reveals an insightful exploration-exploitation-precision trade-off for robust black-box optimization. The TVR can further accommodate a broad class of non-Gaussian distributions on \${\textbackslash}mathcal\{P\}\$ via a careful integration of normalizing flows. We demonstrate the improved performance of TVR over the state-of-the-art in a suite of numerical experiments and an application to the robust design of automobile brake discs under operational uncertainty.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Miller, John Joshua and Mak, Simon},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03816 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ajivani/Zotero/storage/YVZ8D6UR/Miller and Mak - 2024 - Targeted Variance Reduction Robust Bayesian Optim.pdf:application/pdf;arXiv.org Snapshot:/Users/ajivani/Zotero/storage/JLXVCE5L/2403.html:text/html},
}


@misc{Gondu_2024,
	title = {Do {Gaussian} {Processes} scale well with dimension?},
	url = {https://miguelgondu.com/blogposts/2024-03-16/when-does-vanilla-gpr-fail/},
	abstract = {Disputing folk knowledge about how Gaussian Processes scale},
	language = {en},
	urldate = {2024-08-22},
	month = mar,
	year = {2024},
	note = {Section: posts},
}

@misc{eriksson_scalable_2020,
	title = {Scalable {Global} {Optimization} via {Local} {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/1910.01739},
	doi = {10.48550/arXiv.1910.01739},
	abstract = {Bayesian optimization has recently emerged as a popular method for the sample-efficient optimization of expensive black-box functions. However, the application to high-dimensional problems with several thousand observations remains challenging, and on difficult problems Bayesian optimization is often not competitive with other paradigms. In this paper we take the view that this is due to the implicit homogeneity of the global probabilistic models and an overemphasized exploration that results from global acquisition. This motivates the design of a local probabilistic approach for global optimization of large-scale high-dimensional problems. We propose the \${\textbackslash}texttt\{TuRBO\}\$ algorithm that fits a collection of local models and performs a principled global allocation of samples across these models via an implicit bandit approach. A comprehensive evaluation demonstrates that \${\textbackslash}texttt\{TuRBO\}\$ outperforms state-of-the-art methods from machine learning and operations research on problems spanning reinforcement learning, robotics, and the natural sciences.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Eriksson, David and Pearce, Michael and Gardner, Jacob R. and Turner, Ryan and Poloczek, Matthias},
	month = feb,
	year = {2020},
	note = {arXiv:1910.01739 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Appears in NeurIPS 2019 as a spotlight paper},
	file = {arXiv Fulltext PDF:/Users/ajivani/Zotero/storage/H9M3LDEA/Eriksson et al. - 2020 - Scalable Global Optimization via Local Bayesian Op.pdf:application/pdf;arXiv.org Snapshot:/Users/ajivani/Zotero/storage/Q7QUDFK4/1910.html:text/html},
}

@misc{hvarfner_vanilla_2024,
	title = {Vanilla {Bayesian} {Optimization} {Performs} {Great} in {High} {Dimensions}},
	url = {http://arxiv.org/abs/2402.02229},
	doi = {10.48550/arXiv.2402.02229},
	abstract = {High-dimensional problems have long been considered the Achilles' heel of Bayesian optimization algorithms. Spurred by the curse of dimensionality, a large collection of algorithms aim to make it more performant in this setting, commonly by imposing various simplifying assumptions on the objective. In this paper, we identify the degeneracies that make vanilla Bayesian optimization poorly suited to high-dimensional tasks, and further show how existing algorithms address these degeneracies through the lens of lowering the model complexity. Moreover, we propose an enhancement to the prior assumptions that are typical to vanilla Bayesian optimization algorithms, which reduces the complexity to manageable levels without imposing structural restrictions on the objective. Our modification - a simple scaling of the Gaussian process lengthscale prior with the dimensionality - reveals that standard Bayesian optimization works drastically better than previously thought in high dimensions, clearly outperforming existing state-of-the-art algorithms on multiple commonly considered real-world high-dimensional tasks.},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Hvarfner, Carl and Hellsten, Erik Orm and Nardi, Luigi},
	month = jun,
	year = {2024},
	note = {arXiv:2402.02229 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/ajivani/Zotero/storage/UGLYL4AS/Hvarfner et al. - 2024 - Vanilla Bayesian Optimization Performs Great in Hi.pdf:application/pdf;arXiv.org Snapshot:/Users/ajivani/Zotero/storage/XUYY7E98/2402.html:text/html},
}

@misc{brochu_portfolio_2011,
	title = {Portfolio {Allocation} for {Bayesian} {Optimization}},
	url = {http://arxiv.org/abs/1009.5419},
	abstract = {Bayesian optimization with Gaussian processes has become an increasingly popular tool in the machine learning community. It is eÔ¨Écient and can be used when very little is known about the objective function, making it popular in expensive black-box optimization scenarios. It uses Bayesian methods to sample the objective eÔ¨Éciently using an acquisition function which incorporates the model‚Äôs estimate of the objective and the uncertainty at any given point. However, there are several diÔ¨Äerent parameterized acquisition functions in the literature, and it is often unclear which one to use. Instead of using a single acquisition function, we adopt a portfolio of acquisition functions governed by an online multi-armed bandit strategy. We propose several portfolio strategies, the best of which we call GP-Hedge, and show that this method outperforms the best individual acquisition function. We also provide a theoretical bound on the algorithm‚Äôs performance.},
	language = {en},
	urldate = {2024-08-22},
	publisher = {arXiv},
	author = {Brochu, Eric and Hoffman, Matthew W. and de Freitas, Nando},
	month = mar,
	year = {2011},
	note = {arXiv:1009.5419 [cs]},
	keywords = {Computer Science - Machine Learning, G.1.6, G.3, I.2.6},
	annote = {Comment: This revision contains an updated the performance bound and other minor text changes},
	file = {Brochu et al. - 2011 - Portfolio Allocation for Bayesian Optimization.pdf:/Users/ajivani/Zotero/storage/FYQ9MKUT/Brochu et al. - 2011 - Portfolio Allocation for Bayesian Optimization.pdf:application/pdf},
}

@article{letham_constrained_2019,
	title = {Constrained {Bayesian} {Optimization} with {Noisy} {Experiments}},
	volume = {14},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-14/issue-2/Constrained-Bayesian-Optimization-with-Noisy-Experiments/10.1214/18-BA1110.full},
	doi = {10.1214/18-BA1110},
	abstract = {Randomized experiments are the gold standard for evaluating the effects of changes to real-world systems. Data in these tests may be difficult to collect and outcomes may have high variance, resulting in potentially large measurement error. Bayesian optimization is a promising technique for efficiently optimizing multiple continuous parameters, but existing approaches degrade in performance when the noise level is high, limiting its applicability to many randomized experiments. We derive an expression for expected improvement under greedy batch optimization with noisy observations and noisy constraints, and develop a quasi-Monte Carlo approximation that allows it to be efficiently optimized. Simulations with synthetic functions show that optimization performance on noisy, constrained problems outperforms existing methods. We further demonstrate the effectiveness of the method with two real-world experiments conducted at Facebook: optimizing a ranking system, and optimizing server compiler flags.},
	number = {2},
	urldate = {2024-08-22},
	journal = {Bayesian Analysis},
	author = {Letham, Benjamin and Karrer, Brian and Ottoni, Guilherme and Bakshy, Eytan},
	month = jun,
	year = {2019},
	keywords = {Bayesian optimization, quasi-Monte Carlo methods, randomized experiments},
	pages = {495--519},
}
