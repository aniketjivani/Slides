\documentclass[usenames,dvipsnames,aspectratio=169]{beamer}

\mode<presentation> {

% The Beamer class comes with a number of default slide themes
% which change the colors and layouts of slides. Below this is a list
% of all the themes, uncomment each in turn to see what they look like.

%\usetheme{default}
% \usetheme{AnnArbor}
%\usetheme{Antibes}
%\usetheme{Bergen}
% \usetheme{Berkeley}
% \usetheme{Berlin}
%\usetheme{Boadilla}
%\usetheme{CambridgeUS}
% \usetheme{Copenhagen}
\usetheme{Darmstadt}
% \usetheme{Dresden}
% \usetheme{Frankfurt}
% \usetheme{Goettingen}
%\usetheme{Hannover}
%\usetheme{Ilmenau}
%\usetheme{JuanLesPins}
%\usetheme{Luebeck}
% \usetheme{Madrid}
% \usetheme{Malmoe}
%\usetheme{Marburg}
% \usetheme{Montpellier}
% \usetheme{PaloAlto}
% \usetheme{Pittsburgh}
%\usetheme{Rochester}
% \usetheme{Singapore}
%\usetheme{Szeged}
%\usetheme{Warsaw}

% As well as themes, the Beamer class has a number of color themes
% for any slide theme. Uncomment each of these in turn to see how it
% changes the colors of your current slide theme.

% \usecolortheme{albatross}
%\usecolortheme{beaver}
%\usecolortheme{beetle}
%\usecolortheme{crane}
%\usecolortheme{dolphin}
%\usecolortheme{dove}
%\usecolortheme{fly}
\usecolortheme{lily}
%\usecolortheme{orchid}
%\usecolortheme{rose}
%\usecolortheme{seagull}
%\usecolortheme{seahorse}
%\usecolortheme{whale}
%\usecolortheme{wolverine}

%\setbeamertemplate{footline} % To remove the footer line in all slides uncomment this line
%\setbeamertemplate{footline}[page number] % To replace the footer line in all slides with a simple slide count uncomment this line

\setbeamertemplate{navigation symbols}{} % To remove the navigation symbols from the bottom of all slides uncomment this line
}

% % -------------------------------------------
% % Songkai added, feel free to delete --------
% \AtBeginSection[]{
%   \begin{frame}
%   \vfill
%   \centering
%   \begin{beamercolorbox}[sep=8pt,center,shadow=false,rounded=true]{title}
%     \usebeamerfont{title}\insertsectionhead\par%
%   \end{beamercolorbox}
%   \vfill
%   \end{frame}
% }
% % Songkai added, feel free to delete --------
% % -------------------------------------------




\usepackage{graphicx} % Allows including images
\usepackage{booktabs} % Allows the use of \toprule, \midrule and \bottomrule in tables
\usepackage{natbib}
\usepackage{amsmath, amssymb, graphicx, url}
\usepackage[ruled]{algorithm2e}
\usepackage{commath}
\usefonttheme[onlymath]{serif}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{centernot}
\usepackage{comment}
%\usepackage[a4paper, margin=0.8in]{geometry}
\usepackage{parskip}
\usepackage{graphicx}

\usepackage{natbib}

\usepackage{tikz}
\usepackage{tikzlings}

\usepackage{tabularx}
\usepackage{array}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{mathtools}
\usepackage{bm,upgreek}
\usepackage{subcaption}
\usepackage{textpos}
% \usepackage{eso-pic}

% \usepackage{multimedia}
\usepackage{media9}

\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\DeclareUnicodeCharacter{2212}{−}
\usepgfplotslibrary{groupplots,dateplot}
\usetikzlibrary{patterns,shapes.arrows}
\pgfplotsset{compat=newest}



\def\E{\mathbf{E}}
\def\PP{\mathbf{P}}
\def\Reals{\mathbb{R}}
\def\Naturals{\mathbb{N}}
\def\argmin{\operatornamewithlimits{arg\,min}}
\def\deq{:=}
\def\wh#1{\widehat{#1}}
\def\bd#1{\mathbf{#1}}
\def\bx{\bd{x}}
\def\by{\bd{y}}
\def\bZ{\bd{Z}}
\def\bB{\bd{B}}
\def\bV{\bd{V}}
\def\tO{{\tilde{\cO}}}
\def\tOm{\tilde{\Omega}}
\def\barw{\overline{w}}
\def\d{{\mathrm d}}
\def\ave#1{\langle #1 \rangle}
\def\Ave#1{\left\langle #1 \right\rangle}
\def\eps{\varepsilon}
\def\tr{\mathrm{Tr}}


\def\HS{\mathbb{H}}
\def\reals{\mathbb{R}}
\def\ths{\theta^*}
\def\thh{\hat{\theta}}
\def\lbr{\left[}
\def\rbr{\right]}
\def\lc{\left(}
\def\rc{\right)}


    \def\ddefloop#1{\ifx\ddefloop#1\else\ddef{#1}\expandafter\ddefloop\fi}
    % \cA, \cB, ...
    \def\ddef#1{\expandafter\def\csname c#1\endcsname{\ensuremath{\mathcal{#1}}}}
    \ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop
    \def\argmin{\operatornamewithlimits{arg\,min}}
    \def\E{\mathbf{E}}
    \def\bx{\bd{x}}
	\def\by{\bd{y}}
    \def\bZ{\bd{Z}}

\newcommand{\propnumber}{} % initialize
\newtheorem*{prop}{Proposition \propnumber}
\newenvironment{propc}[1]
  {\renewcommand{\propnumber}{#1}%
   \begin{shaded}\begin{prop}}
  {\end{prop}\end{shaded}}

\newcommand{\crlrnumber}{} % initialize
%\newtheorem*{corollary}{Corollary \crlrnumber}
\newenvironment{corollaryc}[1]
  {\renewcommand{\crlrnumber}{#1}%
   \begin{shaded}\begin{corollary}}
  {\end{corollary}\end{shaded}}

\theoremstyle{definition}
% \newtheorem{definition}


\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber
}

% \setbeamertemplate{headline}{% 
%     \leavevmode%
%     \hbox{%
%         \begin{beamercolorbox}[wd=.4\paperwidth,ht=2.25ex,dp=1ex,right]{section in head/foot}%
%             \usebeamerfont{section in head/foot}\insertshorttitle\hspace*{2ex}
%         \end{beamercolorbox}%
%         \begin{beamercolorbox}[wd=.6\paperwidth,ht=2.25ex,dp=1ex,left]{subsection in head/foot}%
%             \usebeamerfont{section in head/foot}\includegraphics[height=2ex,keepaspectratio]{Slides/Block_M-Hex.png}\hspace*{2ex}\insertsectionhead
%         \end{beamercolorbox}%
%     }
% }

% \addtobeamertemplate{headline}{}{%
% \begin{textblock*}{100mm}(.85\textwidth,-1cm)
% \Huge\textcolor{white}{\textbf{\TeX}}
% \end{textblock*}}



%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

%%%% TRY WITH TEXTPOS
% \newcommand{\imgblock}{\begin{textblock*}{5cm}(10.5cm,-1.2cm) % {block width} (coords)
%         \includegraphics[width=1cm]{Slides/Block_M-Hex.png} % loading the image
%     \end{textblock*}
%     }

% \addtobeamertemplate{background}{\imgblock}{}



% \setbeamertemplate{headline}{\hfill\includegraphics[width=1.5cm]{Slides/Block_M-Hex.png}\hspace{0.2cm}\vspace{-1cm}}

% \logo{\includegraphics[height=1cm]{Slides/Block_M-Hex.png}}

\addtobeamertemplate{frametitle}{}{%
    % \begin{textblock*}{5cm}(10.5cm, -0.8cm)
    \begin{textblock*}{5cm}(13.5cm, -0.8cm)
        \includegraphics[width=0.9cm]{Block_M-Hex.png} % your logo file here
    \end{textblock*}
}

\definecolor{mycolor}{cmyk}{100, 60, 0, 60}
\definecolor{my_maize}{rgb}{0.9608,0.7137,0.2588}
\definecolor{my_yellow}{rgb}{0.9294,0.8196,0.2706}

% \setbeamercolor{section in head/foot}{fg=cyan}
\setbeamercolor{section in head/foot}{fg=white}

\setbeamercolor{frametitle}{bg=mycolor}
\setbeamercolor{titlelike}{fg=black, bg=yellow}

\setbeamercolor{section in toc}{fg=black} % Change section color
\setbeamercolor{subsection in toc}{fg=black} % Change subsection color

\setbeamercolor{footline}{fg=blue}
\setbeamerfont{footline}{series=\bfseries}


\usepackage{url}
\usepackage{hyperref}

\usepackage{xcolor}

\hypersetup{pdfauthor={Name},
            colorlinks=true,
            linkcolor={my_yellow},
            % citecolor={blue},
            % linkcolor=[RGB]{0.949, 0.784, 0.035}
            }

% fix inconsistent colors in cite parenthesis (where the closing parenthesis were black instead of the rest of the citecolor!)
% https://github.com/josephwright/beamer/issues/671
\let\oldcite=\cite
\let\oldcitet=\citet
\let\oldcitep=\citep 
\renewcommand{\citet}[2][]{\textcolor{green}{\oldcitet[#1]{#2}}}
\renewcommand{\citep}[2][]{\textcolor{green}{\oldcitep[#1]{#2}}}
\renewcommand{\cite}[2][]{\textcolor{green}{\oldcite[#1]{#2}}}
            

\title[Seminar]{Bayesian Optimal Experimental Design for Adaptive Training of Neural Network Surrogate Models}
% \title[Seminar]{An Adaptive Bayesian Method for Covariance Estimation in Multifidelity Estimators}

% and Estimation of Predictive Uncertainties
% The short title appears at the bottom of every slide, the full title is only on the title page

\author[AJ]{Aniket Jivani, Xun Huan}
\institute[U-M]{University of Michigan}

\date{\today}

% \AtBeginSection[]
% {
%  \begin{frame}<beamer>
%  \frametitle{Plan}
%  \tableofcontents[currentsection]
%  \end{frame}
% }

\makeatletter
\def\beamer@finalpage{}
\makeatother

\begin{document}

\begin{frame}
\titlepage % Print the title page as the first slide
\vspace{}
\center SIAM CSE 2025
\end{frame}

% \begin{frame}{Clarification: different initialization, 1-2 particles}


% \end{frame}

% \begin{frame}{Clarification: different initialization, 1-2 particles}
     

% \bigskip 

\section[Background]{Background and Motivation}
\begin{frame}{Background: Optimal Experimental Design}

A framework to answer: what design conditions or knobs for running an experiment are the “best”?
   
The notion of best needs to be tailored to the goals of the experimenter. Examples:

\begin{itemize}

\item reduction in uncertainty of free parameters



\item reduction in uncertainty of QoIs based on model parameters

\pause

\item calibration to minimize distance to observed data

\item improve correlations between models of multiple fidelities

and many more $\ldots$
\end{itemize}



\end{frame}


\begin{frame}{Background}
When high-fidelity model evaluations are too expensive, we typically:

\begin{itemize}
    \item build a surrogate model after collecting data in one shot
    
    \item use evaluated surrogate as a replacement for the forward model in future inference tasks.
\end{itemize}

\begin{figure}
    \centering
    % \scalebox{0.07}{\includegraphics{surr_workflow.jpg}}
    \includegraphics[width=0.9\textwidth]{workflow_better.png}
\end{figure}


\end{frame}

\begin{frame}{Objective}
    Can we setup training of surrogate models as an OED problem?

    \textbf{Proposed Approach}:

    - Quantify uncertainty in parameters of surrogate model via Bayesian Inference

    - Iterate between inference and maximization of EIG criterion to select new training points.
    
\end{frame}

\begin{frame}{Proposed Workflow}
    \begin{figure}
        \centering
        \includegraphics[width=0.95\textwidth]{workflow_ours.png}
    \end{figure}
\end{frame}

\section[Bayesian Inference]{Bayesian Inference}
\begin{frame}{Definition}
Quantify and update uncertainties in parameters $\theta$ after collecting new observations via Bayes' rule:
    $$p(w | y) = \frac{p(y | w) p(w)}{p(y)}$$

    \vspace{-0.5cm}
    
    \begin{minipage}{0.45\linewidth}
        \centering
        \resizebox{0.86\textwidth}{!}{\input{prior_bi.tex}}
    \end{minipage}
    \hfill
    \begin{minipage}{0.45\linewidth}
        \centering
        \resizebox{0.86\textwidth}{!}{\input{posterior_bi.tex}} % Second figure
    \end{minipage}
\end{frame}

\begin{frame}{Setting up a BNN}

    \textbf{Variational Inference (VI)}

    Approximate a target distribution $p(w | x, y)$, using a simpler $q^{\ast}_{\phi}(w | x, y)$ from a known family, minimize the KL divergence:

    $$q^{\ast} = \arg \min_{q \in \mathcal{Q}}[D_{\text{KL}}(q || p)]$$

    \begin{itemize}
        \item \textcolor{purple}{[Blundell et al. 2015]} BayesByBackprop: Mean-field VI - maximize evidence lower bound (ELBO) and learn $\phi$ for independent Gaussian distributions on weights via backpropagation.
        
        \item Improved variance reduction for minibatches by Flipout-based VI \textcolor{purple}{[Wen et al., 2018]}
    \end{itemize}    
\end{frame}
\begin{frame}{Stein Variational Gradient Descent}

    \textcolor{purple}{[Liu and Wang, 2016]} Removes restrictive assumptions regarding independence of weights by transporting a set of particles to approximate the true posterior density.

    For $n$ particles initialized from $q(w)$, update positions at every iteration $\ell$ via:
    $$w_i^{\ell+1} \leftarrow w_i^{\ell} + \epsilon \boldsymbol{\hat{\phi}}(w_i^{\ell})$$

    where:
    $$\boldsymbol{\hat{\phi}}^*(w_i^{\ell})=\frac{1}{n}\sum_{i=1}^n (\underbrace{\textcolor{red}{k(w_j^\ell,w_i^\ell)\nabla_{w_j^\ell}\log p(w_j^\ell | G, d)}}_{\text{Push towards high-density regions of target}}+\underbrace{\textcolor{blue}{\nabla_{w_j^\ell}k(w_j^\ell,x_i^\ell)}}_{\text{Repulsive force preventing mode collapse}})$$

\end{frame}

\begin{frame}{Example of SVGD}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\linewidth]{GaussianDensity.png}        
    \end{figure}
\end{frame}

\section[OED]{Optimal Experimental Design}
\begin{frame}{Setup for OED}
    \begin{equation}
        y = G(\theta_{\textrm{model}}, d) + \varepsilon
        \label{eq: general_obs_model} \nonumber
    \end{equation}
    
    \begin{equation}
        G = \textcolor{red}{\tilde{G}(\theta_{\textrm{model}}, d, w)} + \eta \nonumber
        \label{eq: general_surr_model}
    \end{equation}

$y, \epsilon$: observations and noise model

$G, d$: forward model and design variables

$\theta, w$: weights of forward model and surrogate

$\tilde{G}$: surrogate model parametrized by $w$

$\eta$: Discrepancy term
\end{frame}

\begin{frame}{Choosing $\eta$}

Since we assume no stochasticity in our simulator output, a reasonable choice for the discrepancy is:

\begin{equation}
    \eta \sim \mathcal{G}\mathcal{P}(0, K_\phi((\theta_{\textrm{model}}, d), (\theta_{\textrm{model}}', d'))) \nonumber
\end{equation}

We use a GP with hyperparameters $\phi$ on the covariance kernel. This effectively models the residual or correction term and reduces the uncertainty near the training data.
    
\end{frame}

\begin{frame}{EIG on Surrogate Model Parameters}
    For networks built with training snapshots varying with $d$, select $d$ for next experiment via: 


    $$d^{\ast} = \arg \max_{d} U(d) = \arg \max_{d} \mathbb{E}_{G | d} \left[ D_{\text{KL}} \left(p(w | G, d) || p(w) \right)\right]$$

    \begin{align*}
        U(d) &= \int_{\mathcal{G}} \int_{\mathcal{W}} \ln \left[\frac{p(w | G, d)}{p(w)}\right]  p(G, w | d) dG dw \nonumber \\
        &= \int_{\mathcal{G}} \int_{\mathcal{W}} \left[\ln[p(G| w, d)] - \ln[p(G | w)]\right] p(G | w, d) p(w) dw dG \nonumber \\
        &\approx \frac{1}{N_{\textrm{out}}} \sum_{i=1}^{N_{\textrm{out}}}\left[\ln[p(G^{(i)} | w^{(i)}, d] - \ln \left[\frac{1}{N_{\textrm{in}}}\sum_{j=1}^{N_{\textrm{in}}}(p(G^{(i)} | w^{(i, j)}, d))\right]\right] \nonumber \\
        \label{eq: util} \nonumber
    \end{align*}

    At every stage, we:
    \begin{enumerate}
        \item Select a batch of optimal designs. For a single design, evaluate DNMC estimator on a fine grid, for larger batches, run Bayesian Optimization on select evaluations of $U$.
        
        \item Warm start BNN training with augmented designs and true labels.
    \end{enumerate}

\end{frame}


\section{Example}
\begin{frame}{Posterior comparisons}
\begin{equation*}
    G(d) = 0.4 \sin(4d) + 0.5 \cos(12d)
\end{equation*}

$\tilde{G}$ is a 2-hidden layer network with 8 units each.
\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{post_predictive_pilot.png}
    \caption{Results from BNN training with MFVI and SVGD on pilot set.}
\end{figure}
\end{frame}

\begin{frame}{SVGD Posterior on Weights}
    \begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{svgd_pilot_post.png}        
    \end{figure}
\end{frame}

\begin{frame}{Retraining with Acquired Points}
    \begin{figure}
        \centering
        \includegraphics[width=0.9\textwidth]{bt1_retrained.png}        
    \end{figure}
\end{frame}

\begin{frame}{}
    \begin{figure}
        \centering
        \includegraphics[width=0.7\textwidth]{post_predictive_2.png}        
    \end{figure}


The quality of the fitted NN strongly depends on the pilot training set - here it smoothly interpolates in the middle of the domain where we have gaps in the training data.
\end{frame}

\begin{frame}{Initializing with a Different Pilot Set}
    \begin{figure}[H]
        \includegraphics[width=0.85\textwidth]{post_predictive_center_point.png}
    \end{figure}
\end{frame}

\begin{frame}{Batch to batch weight posteriors}
    \begin{figure}
        \centering
        % \includegraphics[width=1.1\textwidth]{weight_uncertainties_b2b.png}        

        \includegraphics[width=0.9\textwidth]{batch_posterior_comparisons.png}        
    \end{figure}
\end{frame}

% \begin{frame}{Comparison to randomly acquired training points}

    
% \end{frame}




% \begin{frame}{Goal-oriented Design}
    
% \end{frame}


% \begin{frame}{Model-based Sensitivity Analysis}

% \end{frame}


% \bigskip
% % Now, onto other interesting aspects of Stein-flavored math!
% \end{frame}








% \begin{frame}{Toy Examples}
%     \begin{figure}[H]
%         \centering
%         \includegraphics[width=0.8\linewidth]{NealsFunnel.png}
%         \caption{Toy Problem 2 - Neals Funnel}        
%     \end{figure}
% \end{frame}





% \begin{frame}{Loosely related works}
%     Things that might bear some resemblance to this, but will require more careful reading:

%     - Learning CDFs in an ACV framework -  \url{https://arxiv.org/abs/2303.06422}

%     - HMC swindles - ``chains targeting slightly different stationary distributions still couple approximately when driven by the same randomness'' - antithetic sampling and control variates - \url{https://proceedings.mlr.press/v108/piponi20a/piponi20a.pdf}

%     - Stein control variates - \url{https://arxiv.org/pdf/1710.11198}

%     - Generalized Stein's identity for vector-valued control variates - \url{https://proceedings.mlr.press/v202/sun23a/sun23a.pdf}

    
% \end{frame}




% \begin{frame}{References}
%   \begin{enumerate}
%     \item Kubokawa, T. Stein's identities and related topics. \url{https://doi.org/10.1007/s42081-023-00239-6}
    
%     \item Kattumannil, S.K. On Stein's Identity and its Application. \url{https://www.isid.ac.in/~statmath/2008/isid200805.pdf}
%   \end{enumerate}
% \end{frame}

% \begin{frame}
    
% \end{frame}
\begin{frame}{Setting up goal-oriented design}

    Assuming $z=H(\tilde{G}(w, d^{\ast}))$ i.e. evaluating a function of the NN predictions at a fixed $d$, the MC estimator for GO-OED is (similar to formulation of \textcolor{purple}{Zhong et al. 2024}):
    \begin{align*}
        U(d) &= \int_{\mathcal{G}} \int_{\mathcal{Z}} p(z | G, d) \ln \frac{p(z | G, d)}{p(z)}p(G | d) dz dG \\ \nonumber
        &= \int_{\mathcal{G}} \int_{\mathcal{Z}} \ln \frac{p(z | G, \theta)}{p(z)}p(z, G | \theta) dz dG \nonumber\\
        &= \int_{\mathcal{G}} \int_{\mathcal{Z}} \int_{\mathcal{W}} \ln \frac{p(z | G, \theta)}{p(z)} p(w, G, z | d) dw dz dG \nonumber\\
        &=\int_{\mathcal{G}} \int_{\mathcal{Z}} \int_{\mathcal{W}} \ln \frac{p(z | G, d)}{p(z)} p(G | w) p(w | G, d)p(z | w) dw dz dG \nonumber \\
        &\approx \frac{1}{N_{\textrm{out}}} \sum_{i=1}^{N_{\textrm{out}}} \left[\frac{1}{N_{\textrm{in}}}\sum_{i=1}^{N_{\textrm{in}}} \ln \left[p(z^{(i, j)} | G^{(i)}, d)\right] - \ln\left[p(z^{(i)})\right]\right] \label{eq: estimator_eig_pred} \nonumber
    \end{align*}
\end{frame}

\begin{frame}{SVGD-based GO-OED}
    \begin{enumerate}
        \item Prior samples $w^{(i)}$ can be perturbed to generate initial state of particles for inner loop rather than reusing samples from outer loop; to speed up generation of posterior samples.
        
        \item PDFs of prior predictive $p(z^{(i)})$ and posterior predictive $p(z^{(i, j)})$ are estimated via KDE.
    \end{enumerate}
\end{frame}

\begin{frame}{Summary of Prior and Posterior Predictive}
$$z = \tilde{G}(w, d=-0.5)$$
\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{go_oed_prior_post_predictive_samples.png}
\end{figure}
\end{frame}

\begin{frame}{Results of batch design from $U_{\text{param}}$ and $U_{\text{pred}}$}
    \begin{figure}
        \centering
        \includegraphics[width=0.87\textwidth]{go_vs_nongo.png}
    \end{figure}
\end{frame}


\section{Conclusions and Future Work}
\begin{frame}{Conclusions}

\begin{enumerate}
    \item We looked at strategies to adaptively train neural network surrogates.
    
    \item For a small example, we perform OED targetting uncertainty reduction in NN parameters as well as show a limited setup to target downstream QoIs.
    
    \textcolor{purple}{Scaling up the latter will require more efficient EIG estimation in the goal-oriented setting as well as inference methods for BNNs.}
    
\end{enumerate}
\end{frame}

\begin{frame}{Next Steps}
\begin{enumerate}
    \item Adapting trained NNs for goal-oriented designs with prediction model $H(\theta, d)$ \emph{different from but correlated to} the high-fidelity model $G$.

    \item Tailoring kernel functions for common higher-dimensional inputs to NNs like image data, for instance, convolutional GP kernels \textcolor{purple}{[van der Wilk et al. 2017]}
    
    \item Determining resource allocations as a trade-off between surrogate building and subsequent outer-loop tasks.\footnotemark
    
    \footnotetext{For a sneak peek at balancing pilot sampling and estimation for stochastic optimization in multifidelity frameworks, go to \textcolor{red}{Thomas Coons' talk - MS114, Room 106, 4:40 pm}}
\end{enumerate}
\end{frame}

% \begin{frame}[allowframebreaks]
%     \frametitle{References}
%     \bibliographystyle{chicago}

%     %\bibliographystyle{IEEEtran}
%     \bibliography{references}
% \end{frame}

% \section{Backup}

\begin{frame}{}

    \normalsize \textbf{Acknowledgement:}

    This work relates to Department of Navy award 
    N00014-23-1-2735 issued by the Office of Naval Research.


    \normalsize This material is based upon work supported in part by Sandia National Laboratories Advanced Science and Technology Laboratory Directed Research and Development program. Sandia National Laboratories is a multimission laboratory managed and operated by National Technology and Engineering Solutions of Sandia, LLC., a wholly owned subsidiary of Honeywell International, Inc., for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-NA0003525. 

    SIAM Travel Award for CSE25.

    \begin{figure}
        \centering
        \includegraphics[width=0.6\textwidth]{onr_sandia_log.png}
    \end{figure}

\end{frame}

\begin{frame}{}
\begin{center}
    \Large{Thank you!}

    Contact: \textcolor{blue}{\url{ajivani@umich.edu}}
    % {mailto:ajivani@umich.edu}{ajivani@umich.edu}}

    % \footnotetext{\large \textbf{Acknowledgement:}
    % SIAM Travel Award for CSE25. Funding for these awards is provided by SIAM operating funds, donations to the SIAM Travel Student Fund, royalties generously donated by SIAM book authors, and a grant from the U.S. National Science Foundation (NSF). 
    % }
\end{center}
\end{frame}


\begin{frame}{Initializing with a Different Pilot Set}
\begin{figure}[H]
    \includegraphics[width=1.1\textwidth]{post_predictive_center_point.png}
\end{figure}
\end{frame}

\begin{frame}{OED Classification}

    \begin{figure}[H]
        \includegraphics[width=0.6\textwidth]{classification_oed.png}
    \end{figure}

\end{frame}
% Nice 15 minute read on the subject and the philosophy: \url{https://www.stochasticlifestyle.com/how-to-train-interpretable-neural-networks-that-accurately-extrapolate-from-small-data/}

% \end{frame}
% \begin{frame}{Backup 5: Implementation}
% Some other day.
% \end{frame}
\end{document}
