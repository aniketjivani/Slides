@article{barber,
author = {Rina Foygel Barber},
title = {{Is distribution-free inference possible for binary regression?}},
volume = {14},
journal = {Electronic Journal of Statistics},
number = {2},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {3487 -- 3524},
keywords = {adaptive inference, binary regression, distribution-free, nonparametric inference},
year = {2020},
doi = {10.1214/20-EJS1749},
URL = {https://doi.org/10.1214/20-EJS1749}
}

@InProceedings{splitconfpred,
author="Papadopoulos, Harris
and Proedrou, Kostas
and Vovk, Volodya
and Gammerman, Alex",
editor="Elomaa, Tapio
and Mannila, Heikki
and Toivonen, Hannu",
title="Inductive Confidence Machines for Regression",
booktitle="Machine Learning: ECML 2002",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="345--356",
isbn="978-3-540-36755-0"
}

@book{vovek,
author = {Vovk, Vladimir and Gammerman, Alex and Shafer, Glenn},
title = {Algorithmic Learning in a Random World},
year = {2005},
isbn = {0387001522},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg}
}

@misc{chung2021pinball,
      title={Beyond Pinball Loss: Quantile Methods for Calibrated Uncertainty Quantification}, 
      author={Youngseog Chung and Willie Neiswanger and Ian Char and Jeff Schneider},
      year={2021},
      eprint={2011.09588},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{isotonicreg,
author = {Zadrozny, Bianca and Elkan, Charles},
title = {Transforming Classifier Scores into Accurate Multiclass Probability Estimates},
year = {2002},
isbn = {158113567X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/775047.775151},
doi = {10.1145/775047.775151},
booktitle = {Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {694–699},
numpages = {6},
location = {Edmonton, Alberta, Canada},
series = {KDD '02}
}

@inproceedings{histbinning,
author = {Zadrozny, Bianca and Elkan, Charles},
title = {Obtaining Calibrated Probability Estimates from Decision Trees and Naive Bayesian Classifiers},
year = {2001},
isbn = {1558607781},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Eighteenth International Conference on Machine Learning},
pages = {609–616},
numpages = {8},
series = {ICML '01}
}

@inproceedings{distfreecal,
author = {Gupta, Chirag and Podkopaev, Aleksandr and Ramdas, Aaditya},
title = {Distribution-Free Binary Classification: Prediction Sets, Confidence Intervals and Calibration},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {313},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@phdthesis{bunne_neural_2023,
	title = {Neural {Optimal} {Transport} for {Dynamical} {Systems}: {Methods} and {Applications} in {Biomedicine}},
	shorttitle = {Neural {Optimal} {Transport} for {Dynamical} {Systems}},
	url = {http://hdl.handle.net/20.500.11850/633762},
	language = {en},
	urldate = {2023-10-06},
	school = {ETH Zurich},
	author = {Bunne, Charlotte},
	collaborator = {{Krause, Andreas} and {Cuturi, Marco} and {Pelkmans, Lucas} and {Leskovec, Jure}},
	year = {2023},
	doi = {10.3929/ETHZ-B-000633762},
	keywords = {info:eu-repo/classification/ddc/004, info:eu-repo/classification/ddc/570, Data processing, computer science, Life sciences},
}

@misc{amos_input_2017,
	title = {Input {Convex} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1609.07152},
	doi = {10.48550/arXiv.1609.07152},
	abstract = {This paper presents the input convex neural network architecture. These are scalar-valued (potentially deep) neural networks with constraints on the network parameters such that the output of the network is a convex function of (some of) the inputs. The networks allow for efficient inference via optimization over some inputs to the network given others, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and others. In this paper we lay the basic groundwork for these models, proposing methods for inference, optimization and learning, and analyze their representational power. We show that many existing neural network architectures can be made input-convex with a minor modification, and develop specialized optimization algorithms tailored to this setting. Finally, we highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where we show improvement over the existing state of the art in many cases.},
	urldate = {2023-10-06},
	publisher = {arXiv},
	author = {Amos, Brandon and Xu, Lei and Kolter, J. Zico},
	month = jun,
	year = {2017},
	note = {arXiv:1609.07152 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@InProceedings{pmlr-v202-xu23r,
  title = 	 {Sequential Predictive Conformal Inference for Time Series},
  author =       {Xu, Chen and Xie, Yao},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {38707--38727},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/xu23r/xu23r.pdf},
  url = 	 {https://proceedings.mlr.press/v202/xu23r.html},
  abstract = 	 {We present a new distribution-free conformal prediction algorithm for sequential data (e.g., time series), called the <em>sequential predictive conformal inference</em> (SPCI). We specifically account for the nature that time series data are non-exchangeable, and thus many existing conformal prediction algorithms are not applicable. The main idea is to adaptively re-estimate the conditional quantile of non-conformity scores (e.g., prediction residuals), upon exploiting the temporal dependence among them. More precisely, we cast the problem of conformal prediction interval as predicting the quantile of a future residual, given a user-specified point prediction algorithm. Theoretically, we establish asymptotic valid conditional coverage upon extending consistency analyses in quantile regression. Using simulation and real-data experiments, we demonstrate a significant reduction in interval width of SPCI compared to other existing methods under the desired empirical coverage.}
}

@article{xu_conformal_2023,
	title = {Conformal {Prediction} for {Time} {Series}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {https://ieeexplore.ieee.org/document/10121511/},
	doi = {10.1109/TPAMI.2023.3272339},
	urldate = {2023-10-07},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Xu, Chen and Xie, Yao},
	year = {2023},
        volume={45},
        number={10},
	pages = {11575--11587},
}

@misc{kim_predictive_2020,
	title = {Predictive {Inference} {Is} {Free} with the {Jackknife}+-after-{Bootstrap}},
	url = {https://arxiv.org/abs/2002.09025v3},
	abstract = {Ensemble learning is widely used in applications to make predictions in complex decision problems---for example, averaging models fitted to a sequence of samples bootstrapped from the available training data. While such methods offer more accurate, stable, and robust predictions and model estimates, much less is known about how to perform valid, assumption-lean inference on the output of these types of procedures. In this paper, we propose the jackknife+-after-bootstrap (J+aB), a procedure for constructing a predictive interval, which uses only the available bootstrapped samples and their corresponding fitted models, and is therefore "free" in terms of the cost of model fitting. The J+aB offers a predictive coverage guarantee that holds with no assumptions on the distribution of the data, the nature of the fitted model, or the way in which the ensemble of models are aggregated---at worst, the failure rate of the predictive interval is inflated by a factor of 2. Our numerical experiments verify the coverage and accuracy of the resulting predictive intervals on real data.},
	language = {en},
	urldate = {2023-10-09},
	journal = {arXiv.org},
	author = {Kim, Byol and Xu, Chen and Barber, Rina Foygel},
	month = feb,
	year = {2020},
}

@misc{wen_multi-horizon_2017,
	title = {A {Multi}-{Horizon} {Quantile} {Recurrent} {Forecaster}},
	url = {https://arxiv.org/abs/1711.11053v2},
	abstract = {We propose a framework for general probabilistic multi-step time series regression. Specifically, we exploit the expressiveness and temporal nature of Sequence-to-Sequence Neural Networks (e.g. recurrent and convolutional structures), the nonparametric nature of Quantile Regression and the efficiency of Direct Multi-Horizon Forecasting. A new training scheme, *forking-sequences*, is designed for sequential nets to boost stability and performance. We show that the approach accommodates both temporal and static covariates, learning across multiple related series, shifting seasonality, future planned event spikes and cold-starts in real life large-scale forecasting. The performance of the framework is demonstrated in an application to predict the future demand of items sold on Amazon.com, and in a public probabilistic forecasting competition to predict electricity price and load.},
	language = {en},
	urldate = {2023-10-09},
	journal = {arXiv.org},
	author = {Wen, Ruofeng and Torkkola, Kari and Narayanaswamy, Balakrishnan and Madeka, Dhruv},
	month = nov,
	year = {2017},
}

@article{Kovachki2023,
  author  = {Nikola Kovachki and Zongyi Li and Burigede Liu and Kamyar Azizzadenesheli and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar},
  title   = {Neural Operator: Learning Maps Between Function Spaces With Applications to PDEs},
  journal = {Journal of Machine Learning Research},
  year    = {2023},
  volume  = {24},
  number  = {89},
  pages   = {1--97},
  url     = {http://jmlr.org/papers/v24/21-1524.html}
}

@misc{li_fourier_2021,
	title = {Fourier {Neural} {Operator} for {Parametric} {Partial} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2010.08895},
	doi = {10.48550/arXiv.2010.08895},
	abstract = {The classical development of neural networks has primarily focused on learning mappings between finite-dimensional Euclidean spaces. Recently, this has been generalized to neural operators that learn mappings between function spaces. For partial differential equations (PDEs), neural operators directly learn the mapping from any functional parametric dependence to the solution. Thus, they learn an entire family of PDEs, in contrast to classical methods which solve one instance of the equation. In this work, we formulate a new neural operator by parameterizing the integral kernel directly in Fourier space, allowing for an expressive and efficient architecture. We perform experiments on Burgers' equation, Darcy flow, and Navier-Stokes equation. The Fourier neural operator is the first ML-based method to successfully model turbulent flows with zero-shot super-resolution. It is up to three orders of magnitude faster compared to traditional PDE solvers. Additionally, it achieves superior accuracy compared to previous learning-based solvers under fixed resolution.},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Li, Zongyi and Kovachki, Nikola and Azizzadenesheli, Kamyar and Liu, Burigede and Bhattacharya, Kaushik and Stuart, Andrew and Anandkumar, Anima},
	month = may,
	year = {2021},
	note = {arXiv:2010.08895 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis},
}

@misc{kidger_hey_2021,
	title = {"{Hey}, that's not an {ODE}": {Faster} {ODE} {Adjoints} via {Seminorms}},
	shorttitle = {"{Hey}, that's not an {ODE}"},
	url = {http://arxiv.org/abs/2009.09457},
	doi = {10.48550/arXiv.2009.09457},
	abstract = {Neural differential equations may be trained by backpropagating gradients via the adjoint method, which is another differential equation typically solved using an adaptive-step-size numerical differential equation solver. A proposed step is accepted if its error, {\textbackslash}emph\{relative to some norm\}, is sufficiently small; else it is rejected, the step is shrunk, and the process is repeated. Here, we demonstrate that the particular structure of the adjoint equations makes the usual choices of norm (such as \$L{\textasciicircum}2\$) unnecessarily stringent. By replacing it with a more appropriate (semi)norm, fewer steps are unnecessarily rejected and the backpropagation is made faster. This requires only minor code modifications. Experiments on a wide range of tasks -- including time series, generative modeling, and physical control -- demonstrate a median improvement of 40\% fewer function evaluations. On some problems we see as much as 62\% fewer function evaluations, so that the overall training time is roughly halved.},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Kidger, Patrick and Chen, Ricky T. Q. and Lyons, Terry},
	month = may,
	year = {2021},
	note = {arXiv:2009.09457 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Classical Analysis and ODEs},
}

@misc{chen2019neural,
      title={Neural Ordinary Differential Equations}, 
      author={Ricky T. Q. Chen and Yulia Rubanova and Jesse Bettencourt and David Duvenaud},
      year={2019},
      eprint={1806.07366},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Raissi2019,
title = {Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
journal = {Journal of Computational Physics},
volume = {378},
pages = {686-707},
year = {2019},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
author = {M. Raissi and P. Perdikaris and G.E. Karniadakis},
keywords = {Data-driven scientific computing, Machine learning, Predictive modeling, Runge–Kutta methods, Nonlinear dynamics},
abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.}
}

@article{Zhu2018,
title = {Bayesian deep convolutional encoder–decoder networks for surrogate modeling and uncertainty quantification},
journal = {Journal of Computational Physics},
volume = {366},
pages = {415-447},
year = {2018},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2018.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0021999118302341},
author = {Yinhao Zhu and Nicholas Zabaras},
keywords = {Uncertainty quantification, Bayesian neural networks, Convolutional encoder–decoder networks, Deep learning, Porous media flows},
abstract = {We are interested in the development of surrogate models for uncertainty quantification and propagation in problems governed by stochastic PDEs using a deep convolutional encoder–decoder network in a similar fashion to approaches considered in deep learning for image-to-image regression tasks. Since normal neural networks are data-intensive and cannot provide predictive uncertainty, we propose a Bayesian approach to convolutional neural nets. A recently introduced variational gradient descent algorithm based on Stein's method is scaled to deep convolutional networks to perform approximate Bayesian inference on millions of uncertain network parameters. This approach achieves state of the art performance in terms of predictive accuracy and uncertainty quantification in comparison to other approaches in Bayesian neural networks as well as techniques that include Gaussian processes and ensemble methods even when the training data size is relatively small. To evaluate the performance of this approach, we consider standard uncertainty quantification tasks for flow in heterogeneous media using limited training data consisting of permeability realizations and the corresponding velocity and pressure fields. The performance of the surrogate model developed is very good even though there is no underlying structure shared between the input (permeability) and output (flow/pressure) fields as is often the case in the image-to-image regression models used in computer vision problems. Studies are performed with an underlying stochastic input dimensionality up to 4225 where most other uncertainty quantification methods fail. Uncertainty propagation tasks are considered and the predictive output Bayesian statistics are compared to those obtained with Monte Carlo estimates.}
}

@misc{massaroli_dissecting_2021,
	title = {Dissecting {Neural} {ODEs}},
	url = {http://arxiv.org/abs/2002.08071},
	abstract = {Continuous deep learning architectures have recently re-emerged as Neural Ordinary Differential Equations (Neural ODEs). This infinite-depth approach theoretically bridges the gap between deep learning and dynamical systems, offering a novel perspective. However, deciphering the inner working of these models is still an open challenge, as most applications apply them as generic black-box modules. In this work we "open the box", further developing the continuous-depth formulation with the aim of clarifying the influence of several design choices on the underlying dynamics.},
	urldate = {2023-10-19},
	publisher = {arXiv},
	author = {Massaroli, Stefano and Poli, Michael and Park, Jinkyoo and Yamashita, Atsushi and Asama, Hajime},
	month = Jan,
	year = {2021},
	note = {arXiv:2002.08071 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{anzalone_improving_2022,
	title = {Improving parametric neural networks for high-energy physics (and beyond)},
	volume = {3},
	issn = {2632-2153},
	url = {https://iopscience.iop.org/article/10.1088/2632-2153/ac917c},
	doi = {10.1088/2632-2153/ac917c},
	number = {3},
	urldate = {2023-08-24},
	journal = {Machine Learning: Science and Technology},
	author = {Anzalone, Luca and Diotalevi, Tommaso and Bonacorsi, Daniele},
	month = sep,
	year = {2022},
	pages = {035017},
}

@article{lee_parameterized_2021,
	title = {Parameterized neural ordinary differential equations: applications to computational physics problems},
	volume = {477},
	issn = {1364-5021, 1471-2946},
	shorttitle = {Parameterized neural ordinary differential equations},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2021.0162},
	doi = {10.1098/rspa.2021.0162},
	language = {en},
	number = {2253},
	urldate = {2023-08-24},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Lee, Kookjin and Parish, Eric J.},
	month = sep,
	year = {2021},
	pages = {20210162},
}

@misc{grathwohl_ffjord:_2018,
	title = {{FFJORD}: {Free}-form {Continuous} {Dynamics} for {Scalable} {Reversible} {Generative} {Models}},
	shorttitle = {{FFJORD}},
	url = {http://arxiv.org/abs/1810.01367},
	doi = {10.48550/arXiv.1810.01367},
	urldate = {2023-08-04},
	publisher = {arXiv},
	author = {Grathwohl, Will and Chen, Ricky T. Q. and Bettencourt, Jesse and Sutskever, Ilya and Duvenaud, David},
	month = oct,
	year = {2018},
	note = {arXiv:1810.01367 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
}

@inproceedings{finlay_how_2020,
	series = {{ICML}'20},
	title = {How to train your neural {ODE}: the world of {Jacobian} and {Kinetic} regularization},
	volume = {119},
	shorttitle = {How to train your neural {ODE}},
	urldate = {2023-08-04},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Finlay, Chris and Jacobsen, Jörn-Henrik and Nurbekyan, Levon and Oberman, Adam M},
	month = jul,
	year = {2020},
	pages = {3154--3164},
}

@misc{sitzmann_implicit_2020,
	title = {Implicit {Neural} {Representations} with {Periodic} {Activation} {Functions}},
	url = {http://arxiv.org/abs/2006.09661},
	doi = {10.48550/arXiv.2006.09661},
	abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or Sirens, are ideally suited for representing complex natural signals and their derivatives. We analyze Siren activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how Sirens can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine Sirens with hypernetworks to learn priors over the space of Siren functions.},
	urldate = {2023-10-27},
	publisher = {arXiv},
	author = {Sitzmann, Vincent and Martel, Julien N. P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
	month = jun,
	year = {2020},
	note = {arXiv:2006.09661 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@inproceedings{Norcliffe2020,
 author = {Norcliffe, Alexander and Bodnar, Cristian and Day, Ben and Simidjievski, Nikola and Li\'{o}, Pietro},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {5911--5921},
 publisher = {Curran Associates, Inc.},
 title = {On Second Order Behaviour in Augmented Neural ODEs},
 url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/418db2ea5d227a9ea8db8e5357ca2084-Paper.pdf},
 volume = {33},
 year = {2020}
}

@misc{dupont_augmented_2019,
	title = {Augmented {Neural} {ODEs}},
	url = {http://arxiv.org/abs/1904.01681},
	doi = {10.48550/arXiv.1904.01681},
	abstract = {We show that Neural Ordinary Differential Equations (ODEs) learn representations that preserve the topology of the input space and prove that this implies the existence of functions Neural ODEs cannot represent. To address these limitations, we introduce Augmented Neural ODEs which, in addition to being more expressive models, are empirically more stable, generalize better and have a lower computational cost than Neural ODEs.},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Dupont, Emilien and Doucet, Arnaud and Teh, Yee Whye},
	month = oct,
	year = {2019},
	note = {arXiv:1904.01681 [cs, stat]},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@misc{kidger_neural_2022,
	title = {On {Neural} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2202.02435},
	doi = {10.48550/arXiv.2202.02435},
	abstract = {The conjoining of dynamical systems and deep learning has become a topic of great interest. In particular, neural differential equations (NDEs) demonstrate that neural networks and differential equation are two sides of the same coin. Traditional parameterised differential equations are a special case. Many popular neural network architectures, such as residual networks and recurrent networks, are discretisations. NDEs are suitable for tackling generative problems, dynamical systems, and time series (particularly in physics, finance, ...) and are thus of interest to both modern machine learning and traditional mathematical modelling. NDEs offer high-capacity function approximation, strong priors on model space, the ability to handle irregular data, memory efficiency, and a wealth of available theory on both sides. This doctoral thesis provides an in-depth survey of the field. Topics include: neural ordinary differential equations (e.g. for hybrid neural/mechanistic modelling of physical systems); neural controlled differential equations (e.g. for learning functions of irregular time series); and neural stochastic differential equations (e.g. to produce generative models capable of representing complex stochastic dynamics, or sampling from complex high-dimensional distributions). Further topics include: numerical methods for NDEs (e.g. reversible differential equations solvers, backpropagation through differential equations, Brownian reconstruction); symbolic regression for dynamical systems (e.g. via regularised evolution); and deep implicit models (e.g. deep equilibrium models, differentiable optimisation). We anticipate this thesis will be of interest to anyone interested in the marriage of deep learning with dynamical systems, and hope it will provide a useful reference for the current state of the art.},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Kidger, Patrick},
	month = feb,
	year = {2022},
	note = {arXiv:2202.02435 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Classical Analysis and ODEs, Mathematics - Dynamical Systems, Mathematics - Numerical Analysis, Statistics - Machine Learning},
}

@misc{jia_neural_2020,
	title = {Neural {Jump} {Stochastic} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1905.10403},
	doi = {10.48550/arXiv.1905.10403},
	abstract = {Many time series are effectively generated by a combination of deterministic continuous flows along with discrete jumps sparked by stochastic events. However, we usually do not have the equation of motion describing the flows, or how they are affected by jumps. To this end, we introduce Neural Jump Stochastic Differential Equations that provide a data-driven approach to learn continuous and discrete dynamic behavior, i.e., hybrid systems that both flow and jump. Our approach extends the framework of Neural Ordinary Differential Equations with a stochastic process term that models discrete events. We then model temporal point processes with a piecewise-continuous latent trajectory, where the discontinuities are caused by stochastic events whose conditional intensity depends on the latent state. We demonstrate the predictive capabilities of our model on a range of synthetic and real-world marked point process datasets, including classical point processes (such as Hawkes processes), awards on Stack Overflow, medical records, and earthquake monitoring.},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Jia, Junteng and Benson, Austin R.},
	month = jan,
	year = {2020},
	note = {arXiv:1905.10403 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sun_surrogate_2020,
	title = {Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data},
	volume = {361},
	issn = {00457825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S004578251930622X},
	doi = {10.1016/j.cma.2019.112732},
	language = {en},
	urldate = {2023-11-08},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Sun, Luning and Gao, Han and Pan, Shaowu and Wang, Jian-Xun},
	month = apr,
	year = {2020},
	pages = {112732},
}

@article{ren_phycrnet:_2022,
	title = {{PhyCRNet}: {Physics}-informed convolutional-recurrent network for solving spatiotemporal {PDEs}},
	volume = {389},
	issn = {00457825},
	shorttitle = {{PhyCRNet}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782521006514},
	doi = {10.1016/j.cma.2021.114399},
	language = {en},
	urldate = {2023-11-08},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Ren, Pu and Rao, Chengping and Liu, Yang and Wang, Jian-Xun and Sun, Hao},
	month = feb,
	year = {2022},
	pages = {114399},
}

@article{gao_phygeonet:_2021,
	title = {{PhyGeoNet}: {Physics}-informed geometry-adaptive convolutional neural networks for solving parameterized steady-state {PDEs} on irregular domain},
	volume = {428},
	issn = {00219991},
	shorttitle = {{PhyGeoNet}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999120308536},
	doi = {10.1016/j.jcp.2020.110079},
	language = {en},
	urldate = {2023-11-08},
	journal = {Journal of Computational Physics},
	author = {Gao, Han and Sun, Luning and Wang, Jian-Xun},
	month = mar,
	year = {2021},
	pages = {110079},
}

@article{shafer08,
title = "A tutorial on conformal prediction",
keywords = "confidence, on-line compression modeling, on-line learning, prediction regions",
author = "Glenn Shafer and Vladimir Vovk",
year = "2008",
month = mar,
language = "English",
volume = "9",
pages = "371--421",
journal = "Journal of Machine Learning Research",
url = {https://jmlr.csail.mit.edu/papers/volume9/shafer08a/shafer08a.pdf},
issn = "1532-4435",
publisher = "Microtome Publishing",
}

@misc{angelopoulos_gentle_2022,
	title = {A {Gentle} {Introduction} to {Conformal} {Prediction} and {Distribution}-{Free} {Uncertainty} {Quantification}},
	url = {http://arxiv.org/abs/2107.07511},
	abstract = {Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. Conformal prediction is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions. One can use conformal prediction with any pre-trained model, such as a neural network, to produce sets that are guaranteed to contain the ground truth with a user-specified probability, such as 90\%. It is easy-to-understand, easy-to-use, and general, applying naturally to problems arising in the fields of computer vision, natural language processing, deep reinforcement learning, and so on. This hands-on introduction is aimed to provide the reader a working understanding of conformal prediction and related distribution-free uncertainty quantification techniques with one self-contained document. We lead the reader through practical theory for and examples of conformal prediction and describe its extensions to complex machine learning tasks involving structured outputs, distribution shift, time-series, outliers, models that abstain, and more. Throughout, there are many explanatory illustrations, examples, and code samples in Python. With each code sample comes a Jupyter notebook implementing the method on a real-data example; the notebooks can be accessed and easily run using our codebase.},
	urldate = {2023-08-31},
	publisher = {arXiv},
	author = {Angelopoulos, Anastasios N. and Bates, Stephen},
	month = dec,
	year = {2022},
	note = {arXiv:2107.07511 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Mathematics - Statistics Theory, Statistics - Methodology, Statistics - Machine Learning},
}

@inproceedings{pearce_high-quality_2018,
	title = {High-{Quality} {Prediction} {Intervals} for {Deep} {Learning}: {A} {Distribution}-{Free}, {Ensembled} {Approach}},
	shorttitle = {High-{Quality} {Prediction} {Intervals} for {Deep} {Learning}},
	url = {https://proceedings.mlr.press/v80/pearce18a.html},
	abstract = {This paper considers the generation of prediction intervals (PIs) by neural networks for quantifying uncertainty in regression tasks. It is axiomatic that high-quality PIs should be as narrow as possible, whilst capturing a specified portion of data. We derive a loss function directly from this axiom that requires no distributional assumption. We show how its form derives from a likelihood principle, that it can be used with gradient descent, and that model uncertainty is accounted for in ensembled form. Benchmark experiments show the method outperforms current state-of-the-art uncertainty quantification methods, reducing average PI width by over 10\%.},
	language = {en},
	urldate = {2023-11-15},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Pearce, Tim and Brintrup, Alexandra and Zaki, Mohamed and Neely, Andy},
	month = jul,
	year = {2018},
	pages = {4075--4084},
}

@inproceedings{lakshminarayanan_simple_2017,
	title = {Simple and {Scalable} {Predictive} {Uncertainty} {Estimation} using {Deep} {Ensembles}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/9ef2ed4b7fd2c810847ffa5fa85bce38-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@article{camporeale_accrue:_2021,
	title = {{ACCRUE}: {ACCURATE} {AND} {RELIABLE} {UNCERTAINTY} {ESTIMATE} {IN} {DETERMINISTIC} {MODELS}},
	volume = {11},
	issn = {2152-5080},
	shorttitle = {{ACCRUE}},
	url = {http://www.dl.begellhouse.com/journals/52034eb04b657aea,3ec0b84376cff3d2,1801e97431c5911b.html},
	doi = {10.1615/Int.J.UncertaintyQuantification.2021034623},
	language = {en},
	number = {4},
	urldate = {2023-11-15},
	journal = {International Journal for Uncertainty Quantification},
	author = {Camporeale, Enrico and Carè, Algo},
	year = {2021},
	pages = {81--94},
}

@misc{rackauckas_universal_2021,
	title = {Universal differential equations for scientific machine learning},
	url = {http://arxiv.org/abs/2001.04385},
	doi = {10.48550/arXiv.2001.04385},
	abstract = {In the context of science, the well-known adage "a picture is worth a thousand words" might well be "a model is worth a thousand datasets." In this manuscript we introduce the SciML software ecosystem as a tool for mixing the information of physical laws and scientific models with data-driven machine learning approaches. We describe a mathematical object, which we denote universal differential equations (UDEs), as the unifying framework connecting the ecosystem. We show how a wide variety of applications, from automatically discovering biological mechanisms to solving high-dimensional Hamilton-Jacobi-Bellman equations, can be phrased and efficiently handled through the UDE formalism and its tooling. We demonstrate the generality of the software tooling to handle stochasticity, delays, and implicit constraints. This funnels the wide variety of SciML applications into a core set of training mechanisms which are highly optimized, stabilized for stiff equations, and compatible with distributed parallelism and GPU accelerators.},
	urldate = {2024-01-25},
	publisher = {arXiv},
	author = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali and Edelman, Alan},
	month = nov,
	year = {2021},
	note = {arXiv:2001.04385 [cs, math, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Quantitative Biology - Quantitative Methods, Statistics - Machine Learning},
}